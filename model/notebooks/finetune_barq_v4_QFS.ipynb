{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReX71B-FNqWP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (for speed), and TRL (for training loops)\n",
        "!pip install unsloth\n",
        "# Also install the latest nightly version for GGUF export support\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 1. Configuration for Summarization (High Context, Low Memory)\n",
        "max_seq_length = 16384  # Increased to 16k to read long legal documents\n",
        "dtype = None            # Auto-detect\n",
        "load_in_4bit = True     # Essential for running 16k context on T4\n",
        "\n",
        "# 2. Load the Qwen3-0.6B (The \"Extractor\" Model)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Add LoRA Adapters (High Capacity)\n",
        "# We use r=64 because 0.6B is small; it needs more trainable parameters to master summarization.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,               # Higher rank for complex task\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 128,     # Alpha = 2 * Rank (Aggressive updates)\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"âœ… Qwen3-0.6B Summarizer loaded with 16k context!\")"
      ],
      "metadata": {
        "id": "qyI_DHN3PgqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the file\n",
        "dataset = load_dataset(\"json\", data_files=\"barq_2k_finetune.jsonl\", split=\"train\")\n",
        "\n",
        "# 2. Define the QFS Prompt Template\n",
        "# This forces the model to ignore chat and focus on extraction.\n",
        "summarization_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are a Query-Focused Summarization engine. Extract relevant information from the provided document to answer the user query. Output ONLY the summary. Do not use internal reasoning or conversational filler.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    # Mapping old \"conversations\" format to QFS format\n",
        "    # Input = Document + Query (User Message)\n",
        "    # Output = Summary (Assistant Message)\n",
        "    inputs = [c[1][\"content\"] for c in examples[\"conversations\"]]\n",
        "    outputs = [c[2][\"content\"] for c in examples[\"conversations\"]]\n",
        "\n",
        "    texts = []\n",
        "    for input_text, output_text in zip(inputs, outputs):\n",
        "        # CRITICAL: Remove <think> blocks to train for \"Non-Thinking\" Mode\n",
        "        if \"</think>\" in output_text:\n",
        "            clean_output = output_text.split(\"</think>\")[-1].strip()\n",
        "        else:\n",
        "            clean_output = output_text\n",
        "\n",
        "        # Format: Input contains the doc & query, Output contains the summary\n",
        "        text = summarization_prompt.format(input_text, clean_output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# 3. Apply the format\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "# 4. Verify\n",
        "print(\"\\nğŸ” Training Data Sample (Non-Thinking Summarization):\")\n",
        "print(\"=\"*60)\n",
        "print(dataset[0][\"text\"][:500] + \"...\")"
      ],
      "metadata": {
        "id": "UUNpzS-oS6Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import builtins\n",
        "\n",
        "# Patch the missing library into the system internals\n",
        "builtins.psutil = psutil"
      ],
      "metadata": {
        "id": "HTaykfHeBl9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Set to False for Instruction Tuning\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1, # Keep low for 16k context\n",
        "        gradient_accumulation_steps = 8, # Accumulate to get stable updates\n",
        "        warmup_steps = 50,\n",
        "        max_steps = 750, # Train for roughly 3 epochs on 2k data\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 25,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"barq_summarizer_outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Starting Summarization Training...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Done! The 'Thinking' has been removed.\")"
      ],
      "metadata": {
        "id": "OTy2LpJnToRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 1. Define a Context (Simulating a retrieved document)\n",
        "document = \"\"\"\n",
        "Ø§Ù„Ù…Ø§Ø¯Ø© 50: ÙŠÙ…Ù†Ø¹ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø­Ù…Ø±Ø§Ø¡. Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ùˆ Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ Ø§Ù„Ø±Ø³Ù…ÙŠØ© (Ø¥Ø³Ø¹Ø§ÙØŒ Ø¥Ø·ÙØ§Ø¡) Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù†Ø¨Ù‡Ø§Øª.\n",
        "Ø§Ù„Ù…Ø§Ø¯Ø© 51: ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø§Ø¦Ù‚ÙŠÙ† Ø¥ÙØ³Ø§Ø­ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦.\n",
        "\"\"\"\n",
        "query = \"Ù…ØªÙ‰ ÙŠØ³Ù…Ø­ Ø¨Ù‚Ø·Ø¹ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©ØŸ\"\n",
        "\n",
        "# 2. Format Input\n",
        "# We combine Doc + Query into the 'Input' field\n",
        "input_text = f\"DOCUMENT:\\n{document}\\n\\nQUERY:\\n{query}\"\n",
        "\n",
        "# 3. Use the Training Template\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are a Query-Focused Summarization engine. Extract relevant information from the provided document to answer the user query. Output ONLY the summary. Do not use internal reasoning or conversational filler.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\".format(input_text)\n",
        "\n",
        "# 4. Generate\n",
        "inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 128,  # Short summary\n",
        "    use_cache = True,\n",
        "    temperature = 0.1,     # Low temp for factual extraction\n",
        ")\n",
        "\n",
        "print(\"ğŸ¤– Summary:\")\n",
        "print(tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1].replace(\"<|endoftext|>\", \"\").strip())"
      ],
      "metadata": {
        "id": "BUXfpiW0SVii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to GGUF (Mobile Format)\n",
        "model.save_pretrained_gguf(\n",
        "    \"barq_mobile_v1\",\n",
        "    tokenizer,\n",
        "    quantization_method = \"q4_k_m\"\n",
        ")"
      ],
      "metadata": {
        "id": "_Mjxge9ZX2O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# 1. Ø³Ø¬Ù„ Ø¯Ø®ÙˆÙ„Ùƒ (Ø³ÙŠØ·Ù„Ø¨ Ù…Ù†Ùƒ TokenØŒ Ø§Ø­ØµÙ„ Ø¹Ù„ÙŠÙ‡ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø­Ø³Ø§Ø¨Ùƒ ÙÙŠ Hugging Face)\n",
        "login()\n",
        "\n",
        "# 2. Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù\n",
        "api = HfApi()\n",
        "repo_id = \"y3fai/barq\" # Ø§Ø³ØªØ¨Ø¯Ù„ your-username Ø¨Ø§Ø³Ù…Ùƒ\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ (Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹)\n",
        "api.create_repo(repo_id=repo_id, exist_ok=True, repo_type=\"model\")\n",
        "\n",
        "# Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù\n",
        "print(\"â³ Uploading to Hugging Face...\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"qwen3-1.7b.Q4_K_M.gguf\", # ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø¹Ù†Ø¯Ùƒ\n",
        "    path_in_repo=\"qwen3-1.7b.Q4_K_M.gguf\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "print(\"âœ… Upload Complete! Your model is safe.\")"
      ],
      "metadata": {
        "id": "i_LeZ6xsw_JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCBRnFfWQ44j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}