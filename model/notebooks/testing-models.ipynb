{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf2188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q -U torch transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba08e5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d4ac0f434949db8f6eec053c1191e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e697d7ccdbaf4037a3b93847a9d52880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model loaded on cuda:0 | VRAM used: 1.11 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Load Model in Full Precision (Float16)\n",
    "# This requires approx 4GB VRAM, leaving ~10GB+ for context (long conversations)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # Full half-precision (no 4-bit/8-bit compression)\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Full model loaded on {model.device} | VRAM used: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4bf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3-1.7B (Full Precision) Ready! (THINKING MODE LOCKED ON)\n",
      "Type 'exit' to stop.\n",
      "\n",
      "Qwen (Thinking...): <think>\n",
      "Okay, the user just said \"Ù…Ø±Ø­Ø¨Ø§\" which translates to \"Hello\" in Arabic. I need to respond in Arabic as well. Let me make sure I understand the request correctly. The user might be testing if I can understand Arabic, so I should respond in the same language. I should keep it friendly and welcoming. Let me check the response again to ensure it's correct and matches the user's intent.\n",
      "</think>\n",
      "\n",
      "Ù…Ø±Ø­Ø¨Ø§! ğŸ˜Š\n",
      "Qwen (Thinking...): <think>\n",
      "Okay, the user is asking about their remaining food after having three cakes and one salad. Let me start by understanding their question. They mentioned three cakes and one salad, so they probably have those. Now, they want to know how much they have left. \n",
      "\n",
      "First, I need to check if they are asking about the total amount or if there's something else. The phrase \"Ù…Ø¹ÙŠ Ø«Ù„Ø§Ø« ØªÙØ§Ø­Ø§Øª ÙˆØ§ÙƒÙ„Øª Ø§Ø«Ù†ØªÙŠÙ†\" translates to \"I have three cakes and one salad.\" So, the total is three cakes and one salad. But the user is asking how much is left. Maybe they want to know if there's any leftover. \n",
      "\n",
      "Wait, sometimes people might have multiple dishes and then ask how much is left. But in this case, the question seems straightforward. The user is probably asking how much of their food is still left after the three cakes and one salad. But maybe they meant something else? For example, if they had three cakes and one salad, and then asked how much is left, but maybe there's a translation issue. \n",
      "\n",
      "Alternatively, maybe they are asking if there's any leftovers. But since the original question is in Arabic, maybe there's a nuance. Let me confirm. The user wrote \"Ù…Ø¹ÙŠ Ø«Ù„Ø§Ø« ØªÙØ§Ø­Ø§Øª ÙˆØ§ÙƒÙ„Øª Ø§Ø«Ù†ØªÙŠÙ† ÙƒÙ… Ø¨Ù‚ÙŠ Ù…Ø¹ÙŠØŸ\" which translates to \"I have three cakes and one salad, how much is left?\" So, the answer would be that there are no leftovers, because they ate all of them. But maybe the user is asking if there's any left. \n",
      "\n",
      "Wait, sometimes people might say \"how much is left?\" but if they have eaten all, it's zero. However, maybe the user is confused and wants to know if they have any left. But based on the translation, it's three cakes and one salad. So the answer would be that there are none left. \n",
      "\n",
      "I should make sure the response is clear and in Arabic. Let me check the translation again. \"Ù…Ø¹ÙŠ Ø«Ù„Ø§Ø« ØªÙØ§Ø­Ø§Øª ÙˆØ§ÙƒÙ„Øª Ø§Ø«Ù†ØªÙŠÙ† ÙƒÙ… Ø¨Ù‚ÙŠ Ù…Ø¹ÙŠØŸ\" translates to \"I have three cakes and one salad, how much is left?\" So, the answer should state that there are none left. But maybe the user wants to know if there's any left, so I should say that there are none. \n",
      "\n",
      "Yes, that makes sense. So the final answer would be that there are no leftovers.\n",
      "</think>\n",
      "\n",
      "Ù…Ø±Ø­Ø¨Ø§! ğŸ˜Š  \n",
      "Ù…Ø¹ÙŠ Ø«Ù„Ø§Ø« ØªÙØ§Ø­Ø§Øª ÙˆØ§ÙƒÙ„Øª Ø§Ø«Ù†ØªÙŠÙ† ÙƒÙ… Ø¨Ù‚ÙŠ Ù…Ø¹ÙŠØŸ  \n",
      "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§Ø­Ø§Øª Ø£Ùˆ ÙƒÙ…Ùƒ Ù…Ø¹ÙŠ.\n",
      "Qwen (Thinking...): <think>\n",
      "Okay, the user is asking about the number of legs a snail has. Let me recall. I know that snails are mollusks, and they have two legs. But wait, maybe they're confused because sometimes people think they have more legs. Let me make sure. Snails are typically two legs. So the answer should be two. But I should check if there's any other detail they might be thinking about. No, the question is straightforward. Just confirm that the snail has two legs. Alright, respond with that.\n",
      "</think>\n",
      "\n",
      "Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØª ÙŠØ­Ù…Ù„ 2 Ø±Ø¬Ù„.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TextStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Ensure model and tokenizer are loaded globally first\n",
    "# (If you already ran the loading cell, you don't need to reload them,\n",
    "# but make sure 'tokenizer' and 'model' variables exist)\n",
    "\n",
    "def chat_with_qwen():\n",
    "    print(\"Qwen3-1.7B (Full Precision) Ready! (THINKING MODE LOCKED ON)\")\n",
    "    print(\"Type 'exit' to stop.\\n\")\n",
    "\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Update history\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Apply Template with FORCED Thinking\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True  # <--- HARDCODED TO TRUE\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Initialize the streamer\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "        print(\"Qwen (Thinking...): \", end=\"\")\n",
    "\n",
    "        # Generate\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=4096,\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            streamer=streamer\n",
    "        )\n",
    "\n",
    "        # Decode for history\n",
    "        new_tokens = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "        response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "# Run the chat\n",
    "chat_with_qwen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1ab12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f35ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
